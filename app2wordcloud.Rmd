---
title: "Crie sua própria Word Cloud"
output: 
  flexdashboard::flex_dashboard
runtime: shiny
---



```{r global setup, include=FALSE}
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+                      Importanto pacotes utilizados no aplicativo                             +
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
library(flexdashboard) #Pacote responsavel por gerar o aplicativo como dashboard
library(stringr)       #Pacote para manipulação de strings
library(dplyr)         #Pacote para manipulação de dados
require(tm)            #Pacote de para text mining
require(wordcloud)     #Pacote para nuvem de palavras
require(readxl)        #Pacote para leitura de dados excel
library(tidytext)      #Manipulação de textos
library(reshape2)      #Manipulação de dados
library(lexiconPT)     #Importar palavras de sentimentos
library(memoise)       #Cache resultados de uma função
library(SnowballC)     #Para steamming
library(purrr)         #Ferramentas de programação funcional
library(DT)            #Renderizar tabela da segunda pagina
library(ngram)         #Busca por sequencias de palavras  
#+--------------------------------------+---------------------------------------------+  
#Pacotes desativados:   (Nessa nova versao o RWeka nao sera mais utilizado)
# library(rJava)
# library(RWeka)
#-----------------------------------------------------------------------------------------------

#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+                  Importando fontes nativas do windows para as letras                         +
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# windowsFonts(
#   A=windowsFont("Arial Black"),
#   B=windowsFont("Bookman Old Style"),
#   C=windowsFont("Comic Sans MS"),
#   D=windowsFont("Symbol")
# )
#-----------------------------------------------------------------------------------------------


#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+                         Captação de erros de codificacao:                                    +
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
catch.error = function(x){
  y = NA                                                     # Cria um vetor com valor faltante para teste
  catch_error = tryCatch(tolower(x), error=function(e) e)    # Tente pegar esse erro (NA) que acabamos de criar
  if (!inherits(catch_error, "error"))                       # Se não for um erro
    y = tolower(x)                                           # verificar resultado se houver erro, caso contrário, a função funciona normalmente
  return(y)
}
#Fonte: https://sites.google.com/site/miningtwitter/questions/talking-about/given-topic
#-----------------------------------------------------------------------------------------------


#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+                            Limpeza de caracteres especiais                                   +
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
cleanTweets<- function(tweet){
  
  # Limpe o tweet para análise de sentimentos
  
  tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)  # Remove html links
  tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)       # Remove retweet 
  tweet = gsub("#\\w+", " ", tweet)                             # Remove todos "#Hashtag"
  tweet = gsub("@\\w+", " ", tweet)                             # Remove todos "@people"
  tweet = gsub("[[:punct:]]", " ", tweet)                       # Remove todas as pontuacoes
  tweet = gsub("[[:digit:]]", " ", tweet)                       # Remover numeros, precisamos apenas de texto para análise
  
  tweet = gsub("[ \t]{2,}", " ", tweet)                         # Remove espaços desnecessarios
  tweet = gsub("^\\s+|\\s+$", "", tweet)                        # (espacos em branco, tabs etc)
  
  tweet = gsub('https://','',tweet)                             # Remove https://
  tweet = gsub('http://','',tweet)                              # Remove http://
  tweet = gsub('[^[:graph:]]', ' ',tweet)                       # Remove strings gráficos como emoticons
  tweet = gsub('[[:punct:]]', '', tweet)                        # Remove pontuacao 
  tweet = gsub('[[:cntrl:]]', '', tweet)                        # Remove strings de controle
  tweet = gsub('\\d+', '', tweet)                               # Remove numeros
  tweet=str_replace_all(tweet,"[^[:graph:]]", " ")              # Remove strings gráficos como emoticons
  #tweet=SnowballC::wordStem(tweet,language = "portuguese")     # Aplica steamming (desativado) 

  #Converte tudo para minusculo 
  tweet = catch.error(tweet)                                    # Aplica a funcao catch.error 
  
  return(tweet)
}
#Ref: https://sites.google.com/site/miningtwitter/questions/talking-about/wordclouds/comparison-cloud
#-----------------------------------------------------------------------------------------------

#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+     Funcao Global para tratamento dos dados utilizados em ambas as guias do aplicativo       +
#+                  Fonte: https://shiny.rstudio.com/gallery/word-cloud.html                    +
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
getTermMatrix <- memoise(function(x,                          # Usando "memoise" para armazenar automaticamente os resultados
                                  excludeWords,               # Palavras que sera excluidas
                                  ngrams=1,                   # Ordem da sequencia de palavras
                                  tf_idf=F,                   # Transformacao tf-idf
                                  textStemming=F) {           # Transformacao Steamming
  #Leitura da coluna de texto informada:
  text <- x
  text=as.data.frame(text)
  
  #Funcao para remover acentuacao:
  #-----------------------------------------------------------------------------------------------
  rm_accent <- function(str,pattern="all") {
  # Rotinas e funções úteis V 1.0
  # rm.accent - REMOVE ACENTOS DE PALAVRAS
  # Função que tira todos os acentos e pontuações de um vetor de strings.
  # Parâmetros:
  # str - vetor de strings que terão seus acentos retirados.
  # patterns - vetor de strings com um ou mais elementos indicando quais acentos deverão ser retirados.
  #            Para indicar quais acentos deverão ser retirados, um vetor com os símbolos deverão ser passados.
  #            Exemplo: pattern = c("´", "^") retirará os acentos agudos e circunflexos apenas.
  #            Outras palavras aceitas: "all" (retira todos os acentos, que são "´", "`", "^", "~", "¨", "ç")
  if(!is.character(str))
    str <- as.character(str)
  
  pattern <- unique(pattern)
  
  if(any(pattern=="Ç"))
    pattern[pattern=="Ç"] <- "ç"
  
  symbols <- c(
    acute = "áéíóúÁÉÍÓÚýÝ",
    grave = "àèìòùÀÈÌÒÙ",
    circunflex = "âêîôûÂÊÎÔÛ",
    tilde = "ãõÃÕñÑ",
    umlaut = "äëïöüÄËÏÖÜÿ",
    cedil = "çÇ"
  )
  
  nudeSymbols <- c(
    acute = "aeiouAEIOUyY",
    grave = "aeiouAEIOU",
    circunflex = "aeiouAEIOU",
    tilde = "aoAOnN",
    umlaut = "aeiouAEIOUy",
    cedil = "cC"
  )
  
  accentTypes <- c("´","`","^","~","¨","ç")
  
  if(any(c("all","al","a","todos","t","to","tod","todo")%in%pattern)) # opcao retirar todos
    return(chartr(paste(symbols, collapse=""), paste(nudeSymbols, collapse=""), str))
  
  for(i in which(accentTypes%in%pattern))
    str <- chartr(symbols[i],nudeSymbols[i], str)
  
  return(str)
  }
  #Font: https://pt.stackoverflow.com/questions/46473/remover-acentos
  text=apply(text,1,rm_accent)                               #Aplica no objeto "text, por linha, a funcao definida acima
  #-----------------------------------------------------------------------------------------------
  
  #Criando corpus:
  text=data.frame(doc_id=1:length(text),                     #Criando o data.frame para criar o corpus
                       text=text)
  myCorpus = Corpus(DataframeSource(as.data.frame(text)))    #Criando o corpus a partir de um data.frame
  
  # if(textStemming) myCorpus <- Corpus(VectorSource(text))  #Para stemming (Com atualizacao do pacote tm nao eh mais necessario) 
  
  
  #Tratamento do corpus:
  myCorpus=myCorpus%>%
    tm_map(content_transformer(tolower))%>%                  # Converte para minusculo
    tm_map(removeNumbers)%>%                                 # Remove numeros do corpus
    tm_map(removeWords, stopwords("portuguese"))%>%          # Remove stopwords do dicionario portgues
    tm_map(removePunctuation)%>%                             # Remove pontuacao
    tm_map(stripWhitespace)%>%                               # Remove excessos de espacos em branco
    tm_map(removeWords, excludeWords)                        # Exclue palavras adicionais  
  
  #Realiza o steamming se textStemming==V
  if(textStemming) myCorpus <- tm_map(myCorpus, stemDocument,language="portuguese")
  
  
  #Agora sera feita a procura por sequencias de palavras se ngrams for diferente de 1:

#Abordagem com RWeka (Off)--------------------------------------------------------------------
    # Abordagem antiga utilizava o pacote RWeka, esse pacote nao sera mais utilizado
    # Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = ngrams, max = ngrams))
    # myDTM = TermDocumentMatrix(myCorpus,control = list(tokenize = Tokenizer))
#---------------------------------------------------------------------------------------------
  
  # Se ngrams for difernte de 1:
  if(ngrams!=1){
    temp=ngram::ngram(ngram::concatenate(myCorpus),ngrams)      # Objeto temporario recebe objeto que guarda sequencias
    temp=get.phrasetable(temp)                                  # Obtendo tabela de sequencias do objeto acima
    
    temp$ngrams=temp$ngrams%>%                                  # Limpeza das sequencias obtidas:
      str_replace_all(pattern = "^([A-Za-z] [A-Za-z])+","")%>%  # Remover sequencias de apenas 1 letras 
      str_replace_all(pattern = "[:punct:]","")%>%              # Remover caracteres especiais
      str_replace_all(pattern = "\n","")%>%                     # Remover o marcador de "nova linha"
      str_trim()                                                # Remover espaços em branco sobrando
    
    #Apos a limpeza..
    
    temp=temp[temp$ngrams!="",]                                 # Selecionando apenas as linhas que contenham informacao
    
    temp=temp%>%                                                # Novamente manipulando o objeto que contem a tabela de sequencias
      group_by(ngrams) %>%                                      # Agrupando por "ngrams" (sequencias obtidas)
      summarise(freq=sum(freq))%>%                              # Resumir as linhas repetidas pela soma das frequencias
      arrange(desc(freq))%>%                                    # Organizando da maior para a menos frequencia
      as.matrix()                                               # Alterando o tipo de objeto para matrix
    
    rownames(temp)=str_c(temp[,1])                              # O nome das linhas passa a ser a sequencia correspondente
    sort(temp[,2],decreasing = T)                               # Retorna um objeto com as frequencias em ordem decrescente e linhas nomeadas
    
  # Caso contrario, se ngrams for igual a 1 (sem sequencias)      
  }else{
  
  myDTM = TermDocumentMatrix(myCorpus,                          # Obtendo matriz de termos:
              control = list(minWordLength = 1))                # Com no minimo 1 ocorrencia
  
  #Se se ngrams for igual a 1 (sem sequencias) e tf.idf for verdadeiro:
      if(tf_idf==T){
        myDTM=weightTfIdf(myDTM,normalize=T)            # Realiza transformacao tf-idf
      }
   
  m = as.matrix(myDTM)                        # Transforma o objeto em matrix
  sort(rowSums(m),decreasing=TRUE)            # Retorna um objeto com as frequencias em ordem decrescente e linhas nomeadas
  
  }
  

  
})

```

Criar Wordcloud
=====================================  

Column
--------------------------------------
### Aplicativo para construir nuvem de palavras 



```{r}
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+                         Criando o shinyApp(ui, server)                                       +
#+                                                                                              +
#+ Para aplicativos definidos dessa maneira, o arquivo server.R deve retornar a função do       +
#+ servidor e o arquivo ui.R deve retornar o objeto UI (neste caso, o objeto UI é criado por    +
#+ fluidPage ()). Mais info: https://shiny.rstudio.com/articles/app-formats.html                +    
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# UI --------------------------------------------------------------------------------------------
ui <- fluidPage(                                                      # Funções para criar layouts de páginas "fluidos".
  
  titlePanel("Word Cloud"),                                           # Titulo do aplicativo
  sidebarLayout(                                                      # SidebarPanel contendo controles de entrada
    sidebarPanel(                                                     # sidebarPanel contem os inputs e o mainPanel contém outputs

      
      radioButtons("sep", "Separador",                                # Input: Selecionar um item de uma lista
                   choices = c(Comma = ",",Semicolon = ";",           # Opcoes de separadores do input em formato csv
                               Tab = "\t"),selected = ","),
      
      fileInput("file1", "Escolha o arquivo no formato CSV",          # Input: Selecionar um arquivo csv do computador 
                multiple = TRUE,                                      # Permite carregar mais de um arquivo
                accept = c("text/csv",                                # Tipos de arquivos aceitos
                         "text/comma-separated-values,text/plain",
                         ".csv")),
      
      actionButton("update", "Atualizar nuvem"),                      # Preciona para recarregar a nuvem
      
      
      textInput("text", label = h3("Removendo palavras"),             # Input: Insere strings que considerados stopwords
            value = "inserir stopwords de acordo com instrução"),
      hr(),                                                           # Linha horizontal

#Opcoes para o layout da wordcloud :
      sliderInput("scalemin",                                             # Input: Deslize para selecionar Tamanho minimo para palavras
                  "Tamanho minimo para palavras:",                        # Titulo
                  min = 0.5,  max = 4, value = 0.5),                      # Valor minimo, maximo e default
      sliderInput("scalemax",                                             # Input: Deslize para selecionar Tamanho maximo para palavras
                  "Tamanho maximo para palavras:",                    # Titulo
                  min = 1,  max = 5, value = 4),                      # Valor minimo, maximo e default
      sliderInput("freq",                                             # Input: Deslize para selecionar algum valor no intervalo
                  "Frequência minima:",                               # Titulo
                  min = 1,  max = 100, value = 15),                   # Valor minimo, maximo e default
      sliderInput("max",
                  "Número máximo de palavras na nuvem:",
                  min = 1,  max = 300,  value = 100),
      sliderInput("horizontal",                                       # Posicao das palavras
                "Porcentagem de palavras na horizontal:",
                min = 0,  max = 1, value = 0.65),
      sliderInput("ngrams",
              "Número de sequências de palavras",
                  min = 1,  max = 5,  value = 1),
      hr(),                                                           # Linha horizontal
#Opcoes para analise da wordcloud :
      checkboxInput("header", "Marque se a base possuir cabeçalho", TRUE),              # Cabecalho
      checkboxInput("cor", "Marque se deseja usar cor para sentimentos", TRUE),         # Cor neutra ou com base em sentimentos
      checkboxInput("textStemming",                                                     # Steamming
                    "Marque se deseja usar 'Text Steamming' (remover sufixos)", F),     # tf-idf
      checkboxInput("tf_idf", "Marque se deseja usar a transformação tf-idf", F)
    ),
    

# Para exibir a nuvem :
    mainPanel(                                                        # mainPanel contém outputs e o sidebarPanel contem os inputs 
      plotOutput("plot"),                                             #"plot" = "output$plot <- renderPlot({  (...) })"
      downloadButton("downloadPlot", "Download da nuvem"),          # downloadtable = "output$downloadtable <- downloadHandler( ... )"
      tags$br(),                                                      # Espaço em branco
      tags$hr(),                                                             # Linha horizontal
	  dataTableOutput("table"),                                         #"table" = "output$table <- renderDataTable({ (...) })"
      tags$br(),                                                      # Espaço em branco
      downloadButton("downloadtable", "Download da tabela"),          # downloadtable = "output$downloadtable <- downloadHandler( ... )"
      plotOutput("plot2"),                                             #"plot" = "output$plot <- renderPlot({  (...) })"
      tags$br()                                                      # Espaço em branco
    )
  )
)

# SERVER ---------------------------------------------------------------------------------------

server <- function(input, output, session) {                # Funcao do servidor que recebe o imput$ e retorna o output$              
  
  terms <- reactive({                                       # Definir uma expressão reativa para o do documento
set.seed(12) 
    req(input$file1)                                               # Leitura dos dados ficara armazenada no objeto terms
    df <- read.csv(input$file1$datapath,                    # Leitura da base de dados com os inputs obtidos em ui
             header = input$header,
             sep = input$sep,encoding = "UTF-8")
    df=df[,1:2]
# Removendo linhas duplicadas:
    names(df)=c("V1","V2")                                  # Altera o nome das variaveis da base lida
    df=df%>%                                                # Alterando o objeto lido
      distinct(V2,V1,keep_all=T)                            # Remove linhas iguais de acordo com o texto em seguida o usuario iguais
    df=df[,2]                                               # Seleciona a coluna do texto
    df=apply(data.frame(df),1,cleanTweets)                  # Aplica a funcao cleanTweets

    
    
# Altera quando o botao "update" for precionado:
    input$update
    isolate({                                               # Executa a expressão dada em um escopo em que os valores reativos ou a expressão podem ser lidos
      withProgress({                                                       # Relata o progresso ao usuário durante operações de longa duração
        setProgress(message = "Processando corpus...")                     # Mensagem de processamento
        excludeWords=input$text                                            # Obtendo as stopwords separadas por virgula
        excludeWords=as.vector(str_split(excludeWords, fixed(','))[[1]])   # Cria o vetor com as stopwords informadas
        getTermMatrix(df,                                                  # Executa a funcao global getTermMatrix() responsavel por fazer o tratamento da base de dados
                      excludeWords,
                      ngrams=input$ngrams,
                      tf_idf=input$tf_idf,
                      textStemming=input$textStemming
                      )
      })
    })
  })

# Faça o desenho do wordcloud previsível durante uma sessão
  wordcloud_rep <- repeatable(wordcloud)                                 # Uma versão repetitiva da função que foi informada

#Exibir a tabela com as palavras da wordcloud previsivel durante uma sessao
  datatable_rep <- repeatable(datatable)                                 #Uma versao repetitiva da função que foi informada

    output$plot <- renderPlot({                                          # Gerando o output$plot usado em  mainPanel(plotOutput("plot")) no UI
    set.seed(12) 
    v <- terms()                                                         # Obtendo o resultado da função global "getTermMatrix()"
                                                          # Semente para que a nuvem criada seja sempre a mesma
    
    d <- data.frame(words = names(v),freq=v)                             # Crie um data.frame com as strings e sua freqüência


  output$table <- renderDataTable({                                      # Gerando o output$table usado em mainPanel(dataTableOutput("table"))

    datatable_rep(d,rownames=F)                                          # Utilizando a versao repetitiva da função datatable
    
  })
  
#Incluindo a funcao que gera a base de dados que sera disponivel para download com o botao
  output$downloadtable <- downloadHandler(                               # O output$downloadtable usado em  mainPanel(downloadButton("downloadtable")) no UI
  filename = function() {
    paste('stats', '.csv', sep='')
  },
  content = function(file) {
    set.seed(12)
    v <- terms()
    
    df2 <- data.frame(words = names(v),freq=v)

    write.csv(df2, file,row.names = F)
  }
)
# Fonte: https://stackoverflow.com/questions/45236368/how-to-use-the-download-button-in-shiny
  
   
# Analise de sentimentos
  if(input$cor){                                                         # Se a opcao de sentimentos estiver selecionada
    
      sentiLex_lem_PT02 <- lexiconPT::sentiLex_lem_PT02                  # Obtem o dicionario lexico em Portugues
      
      dicionary=data.frame(cbind(sentiLex_lem_PT02$term,                 # Seleciona as palavras e 
                                 sentiLex_lem_PT02$polarity))            # a polaridade de cada uma
      
      matriz=d                                                           # Cria uma base temporaria para o join
      
      names(dicionary)=c("words", "sentiment")                           # Alterando os nomes das colunas do dicionario e da base obtida
      names(matriz)=c("words", "freq")                                   # (O nome deve ser o mesmo para o join)
       
      dicionary$words=as.character(dicionary$words)                      # Transformando as duas
      matriz$words=as.character(matriz$words)                            # bases obtidas em strings
      
      
      if(input$textStemming){                                            # Se textStemming estiver ligada
        dicionary$words <- wordStem(dicionary$words,                     # Steamming o dicionario lexico tambem
                                    language = "portuguese")             # De acordo com a lingua portuguesa
        }
      
      dicionary=dicionary[ dicionary$sentiment==1 | dicionary$sentiment==0 | dicionary$sentiment==-1, ] # Obtendo so os polos -1, 1 e 0
      dicionary$sentiment=as.factor(dicionary$sentiment)                # Transformando em fator
      
      #Alterando o nome dos fatores para o respectivo sentimento:
      levels(dicionary$sentiment)[levels(dicionary$sentiment)==-1]=c("Negativo")
      levels(dicionary$sentiment)[levels(dicionary$sentiment)==0]=c("Neutro")
      levels(dicionary$sentiment)[levels(dicionary$sentiment)==1]=c("Positivo")
      
      #Join das strings do documento com as strings nativas do pacote LexiconPT
      sentimentos=data.frame(matriz) %>%                                # O objeto "sentimentos" recebe
        left_join(data.frame(dicionary),by="words") %>%                 # left_join mantem todas as linhas da base matriz (esquerda) e busca por pares com o dicionario 
        select(words,sentiment,freq)%>%                                 # Seleciona as colunas das strings, o sentimento e sua frequencia
        distinct(words,.keep_all = T)                                   # Seleciona apenas linhas distintas
      
      rownames(d)=d$words                                               # Nomeia as linhas do data.frame d   
      
      sentimentos$sentiment[is.na(sentimentos$sentiment)]="Neutro"      # Neutro para palavras fora do dicionario
      
      #Criando coluna de cores para cada sentimento
      sentimentos$col=c(                                                # Criando uma nova coluna para objeto "sentimentos"
        ifelse(sentimentos$sentiment=="Neutro","#666666",               # Se for "Neutro" sera cinza, se nao:
               ifelse(sentimentos$sentiment=="Positivo","blue","red"))) #Se for "Positivo" recebe azul, se nao é vermelho
    
    par(bg="#d7dee1")                                                   # Define a cor do fundo da figura
    set.seed(12)
    req(input$file1)                                               # Leitura dos dados ficara armazenada no objeto terms
    wordcloud_rep(names(v), freq=as.numeric(v),                         # Funcao repetitiva wordcloud, objeto v foi retornado pela funcao global getTermMatrix()
                  scale=c(input$scalemax,input$scalemin),                                       # Um vetor de comprimento 2 que indica o range do tamanho das palavras     
                  min.freq = input$freq,                                # Input informado pelo usuario
                  max.words=input$max,                                  # Input informado pelo usuario
                  colors=sentimentos$col,                               # Input informado pelo usuario
                  random.order=FALSE,                                   # Plot das palavras em ordem aleatória. Se falso, eles serão plotados em frequência decrescente
                  rot.per=(1-input$horizontal),                         # Proporção de palavras com rotação de 90 graus
                  use.r.layout=FALSE,                                   # Se falso, o código c ++ é usado para detecção de colisão, caso contrário, R é usado
                  family = "C",                                         # Seleciona a fonte informada em windowsFonts no inicio do documento
                  font = 2)                                             # 1:default, 2:negrito, 3:italico, 4:negrito+italico

#Se a analise de sentimentos nao for selecionada  
    }else{
    par(bg="#d7dee1")                                                   # Define a cor do fundo da figura
      set.seed(12)
      req(input$file1)                                               # Leitura dos dados ficara armazenada no objeto terms
     wordcloud_rep(names(v), freq=as.numeric(v),                        # As entradas serao as mesmas mensionadas acima, com excessao da cor
                   scale=c(input$scalemax,input$scalemin),                                      # Um vetor de comprimento 2 que indica o range do tamanho das palavras   
                   min.freq = input$freq,                               # Input informado pelo usuario
                   max.words=input$max,                                 # Input informado pelo usuario
                   colors=c("#dd003f","#002637","#0bdc99","#00bac5","#ff8947","#c20037","#00a2ab")%>%  #Cores desejadas
                     rev(),                                                        #Altera a ordem que as cores participarao da figura
                   random.order=FALSE,                                  # Plot das palavras em ordem aleatória. Se falso, eles serão plotados em frequência decrescente
                   rot.per=(1-input$horizontal),                        # Proporção de palavras com rotação de 90 graus
                   use.r.layout=FALSE,
                   family = "C",                                        # Seleciona a fonte informada em windowsFonts no inicio do documento 
                   font = 2)                                            # 1:default, 2:negrito, 3:italico, 4:negrito+italico
    }
    
  })                                                                    # Encerra o renderPlot({})
  
    
    
    
    
    
    ###################################################################################################
    ##########    Repetindo o codigo para renderizar a nuvem para download  ###########################
    ###################################################################################################
    
    
    
    
    output$plot2 <- renderPlot({                                          # Gerando o output$plot2 usado em  mainPanel(plotOutput("plot2")) no UI para renderizar a nuvem para download
    
      # criando botao de download para nuvem 
  output$downloadPlot <- downloadHandler(
    filename = "wordcloud.png",
    content = function(file) {
    png(file, width = 800, height = 800, type='cairo')

    set.seed(12)                                                       # Semente para que a nuvem criada seja sempre a mesma  
    v <- terms()                                                         # Obtendo o resultado da função global "getTermMatrix()"
    
    
    d <- data.frame(words = names(v),freq=v)                             # Crie um data.frame com as strings e sua freqüência
  
   
# Analise de sentimentos
  if(input$cor){                                                         # Se a opcao de sentimentos estiver selecionada
    
      sentiLex_lem_PT02 <- lexiconPT::sentiLex_lem_PT02                  # Obtem o dicionario lexico em Portugues
      
      dicionary=data.frame(cbind(sentiLex_lem_PT02$term,                 # Seleciona as palavras e 
                                 sentiLex_lem_PT02$polarity))            # a polaridade de cada uma
      
      matriz=d                                                           # Cria uma base temporaria para o join
      
      names(dicionary)=c("words", "sentiment")                           # Alterando os nomes das colunas do dicionario e da base obtida
      names(matriz)=c("words", "freq")                                   # (O nome deve ser o mesmo para o join)
       
      dicionary$words=as.character(dicionary$words)                      # Transformando as duas
      matriz$words=as.character(matriz$words)                            # bases obtidas em strings
      
      
      if(input$textStemming){                                            # Se textStemming estiver ligada
        dicionary$words <- wordStem(dicionary$words,                     # Steamming o dicionario lexico tambem
                                    language = "portuguese")             # De acordo com a lingua portuguesa
        }
      
      dicionary=dicionary[ dicionary$sentiment==1 | dicionary$sentiment==0 | dicionary$sentiment==-1, ] # Obtendo so os polos -1, 1 e 0
      dicionary$sentiment=as.factor(dicionary$sentiment)                # Transformando em fator
      
      #Alterando o nome dos fatores para o respectivo sentimento:
      levels(dicionary$sentiment)[levels(dicionary$sentiment)==-1]=c("Negativo")
      levels(dicionary$sentiment)[levels(dicionary$sentiment)==0]=c("Neutro")
      levels(dicionary$sentiment)[levels(dicionary$sentiment)==1]=c("Positivo")
      
      #Join das strings do documento com as strings nativas do pacote LexiconPT
      sentimentos=data.frame(matriz) %>%                                # O objeto "sentimentos" recebe
        left_join(data.frame(dicionary),by="words") %>%                 # left_join mantem todas as linhas da base matriz (esquerda) e busca por pares com o dicionario 
        select(words,sentiment,freq)%>%                                 # Seleciona as colunas das strings, o sentimento e sua frequencia
        distinct(words,.keep_all = T)                                   # Seleciona apenas linhas distintas
      
      rownames(d)=d$words                                               # Nomeia as linhas do data.frame d   
      
      sentimentos$sentiment[is.na(sentimentos$sentiment)]="Neutro"      # Neutro para palavras fora do dicionario
      
      #Criando coluna de cores para cada sentimento
      sentimentos$col=c(                                                # Criando uma nova coluna para objeto "sentimentos"
        ifelse(sentimentos$sentiment=="Neutro","#666666",               # Se for "Neutro" sera cinza, se nao:
               ifelse(sentimentos$sentiment=="Positivo","blue","red"))) #Se for "Positivo" recebe azul, se nao é vermelho
    
    par(bg="#d7dee1")                                                   # Define a cor do fundo da figura
    set.seed(12)
    req(input$file1)
    wordcloud_rep(names(v), freq=as.numeric(v),                         # Funcao repetitiva wordcloud, objeto v foi retornado pela funcao global getTermMatrix()
                  scale=c(input$scalemax,input$scalemin),                                       # Um vetor de comprimento 2 que indica o range do tamanho das palavras     
                  min.freq = input$freq,                                # Input informado pelo usuario
                  max.words=input$max,                                  # Input informado pelo usuario
                  colors=sentimentos$col,                               # Input informado pelo usuario
                  random.order=FALSE,                                   # Plot das palavras em ordem aleatória. Se falso, eles serão plotados em frequência decrescente
                  rot.per=(1-input$horizontal),                         # Proporção de palavras com rotação de 90 graus
                  use.r.layout=FALSE,                                   # Se falso, o código c ++ é usado para detecção de colisão, caso contrário, R é usado
                  # family = "C",                                         # Seleciona a fonte informada em windowsFonts no inicio do documento
                  font = 2)                                             # 1:default, 2:negrito, 3:italico, 4:negrito+italico

#Se a analise de sentimentos nao for selecionada  
    }else{
    par(bg="#d7dee1")                                                   # Define a cor do fundo da figura
      set.seed(12)
      req(input$file1)
     wordcloud_rep(names(v), freq=as.numeric(v),                        # As entradas serao as mesmas mensionadas acima, com excessao da cor
                   scale=c(input$scalemax,input$scalemin),                                      # Um vetor de comprimento 2 que indica o range do tamanho das palavras   
                   min.freq = input$freq,                               # Input informado pelo usuario
                   max.words=input$max,                                 # Input informado pelo usuario
                   colors=c("#dd003f","#002637","#0bdc99","#00bac5","#ff8947","#c20037","#00a2ab")%>%  #Cores desejadas
                     rev(),                                                        #Altera a ordem que as cores participarao da figura
                   random.order=FALSE,                                  # Plot das palavras em ordem aleatória. Se falso, eles serão plotados em frequência decrescente
                   rot.per=(1-input$horizontal),                        # Proporção de palavras com rotação de 90 graus
                   use.r.layout=FALSE,
                   # family = "C",                                      # Seleciona a fonte informada em windowsFonts no inicio do documento 
                   font = 2)                                            # 1:default, 2:negrito, 3:italico, 4:negrito+italico
    }
    
  
      dev.off()                                                         # Encerrando dispositivo da nuvem
  },
  contentType = 'application/png'                                       # Formato que deve ser salvo a nuvem
)
  
  })                                                                    # Encerra o renderPlot({})
    
}

#Cria o aplicativo Shinny 
shinyApp(ui, server)



```


Column {data-width=110}
--------------------------------------
### **Instruções**

**Input da base de dados**

Para dar início a construção da nuvem de palavras selecione o arquivo (em formato csv com codificação UTF-8) que contenha apenas duas colunas em que:

    * Coluna1: usuario
    * Coluna2: texto

[exemplo de base](https://github.com/dtpapp/appwordcloud/blob/master/base2.csv)

**Stopwords**

Para remover palavras da nuvem de palavras (chamadas de stopwords), basta inseri-las, separadas por vírgula, na caixa "**Removendo palavras**". Exemplo de como devem ser incluidas as palavras:

     nao, dele, dela, nos

**Sentimentos**

Na opção "**Marque se deseja usar cor para sentimentos**" a cor da nuvem é baseada em um dicionário léxico do qual um conjunto de palavras já foram pré-classificadas como positiva, negativa ou neutra.


[Codigo no github](https://github.com/dtpapp/appwordcloud)

**Importante**: Para utilizar a transformação tf-idf em conjunto com a remoção de sufixos, o procedimento deve ser feito na ordem: 

    Text Stemming > tf-idf

